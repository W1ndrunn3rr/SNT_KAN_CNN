# Analiza problemu: Validation 98% vs Test 35%

## ğŸ” Problem

Model osiÄ…ga **98% accuracy na zbiorze walidacyjnym**, ale tylko **35% na zbiorze testowym**, pomimo Å¼e dane pochodzÄ… z tego samego ÅºrÃ³dÅ‚a.

## ğŸ¯ Przyczyny

### 1. **NieprawidÅ‚owy podziaÅ‚ danych (GÅÃ“WNY PROBLEM)**

**Poprzednia implementacja:**
```python
# âŒ ZÅE - brak ustawionego seed
train_dataset, val_dataset = random_split(self.dataset, [train_size, val_size])
```

**Problemy:**
- Brak seed â†’ za kaÅ¼dym razem inny podziaÅ‚
- Brak reprodukowalnoÅ›ci
- Model moÅ¼e "widzieÄ‡" rÃ³Å¼ne dane walidacyjne przy rÃ³Å¼nych uruchomieniach

### 2. **Brak augmentacji danych**

**Poprzednio:**
- Te same transformacje dla train i validation
- Model nie uczy siÄ™ robustnych cech
- Overfitting na konkretne przykÅ‚ady

### 3. **Brak rÃ³Å¼nicowania train/val transforms**

**Co to oznacza:**
- ZbiÃ³r walidacyjny pochodzi z tych samych rozkÅ‚adÃ³w co treningowy
- Model "zna" statystyki danych treningowych
- Test set moÅ¼e mieÄ‡ inny rozkÅ‚ad (distribution shift)

## âœ… RozwiÄ…zania

### 1. **Poprawiony DataProcessor**

```python
# âœ… DOBRE - z seedem i augmentacjÄ…
generator = torch.Generator().manual_seed(seed)
train_dataset, val_dataset = random_split(
    full_dataset, [train_size, val_size], generator=generator
)
```

**Zalety:**
- Reprodukowalny podziaÅ‚ (seed=42)
- RÃ³Å¼ne transformacje dla train (z augmentacjÄ…) i val (bez augmentacji)
- Lepsze parametry DataLoader (pin_memory, persistent_workers)

### 2. **Augmentacja danych dla treningu**

```python
transforms.RandomHorizontalFlip(p=0.5),
transforms.RandomVerticalFlip(p=0.5),
transforms.RandomRotation(degrees=15),
transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
```

**Dlaczego to pomaga:**
- Model widzi rÃ³Å¼ne wersje tych samych obrazÃ³w
- Uczy siÄ™ bardziej ogÃ³lnych cech
- Redukuje overfitting

### 3. **KonfigurowalnoÅ›Ä‡ przez Hydra**

```yaml
data_params:
  val_split: 0.2  # 20% danych na walidacjÄ™
  seed: 42        # ReprodukowalnoÅ›Ä‡
```

## ğŸ“Š Jak zdiagnozowaÄ‡ problem

### Uruchom analizÄ™ danych:

```bash
just analyze-data
```

**Skrypt sprawdzi:**
- âœ“ RozkÅ‚ad klas w train vs test
- âœ“ Statystyki obrazÃ³w (mean, std, rozmiary)
- âœ“ Data leakage (duplikaty miÄ™dzy zbiorami)
- âœ“ Distribution shift

### Rezultaty analizy:

```
ğŸ“Š Analyzing Training Set: data/train
Total samples: 25600
Classes: ['agri', 'barrenland', 'grassland', 'urban']
Class distribution:
  agri        :   6400 (25.00%)
  barrenland  :   6400 (25.00%)
  grassland   :   6400 (25.00%)
  urban       :   6400 (25.00%)

ğŸ“Š Analyzing Test Set: data/test
Total samples: 6400
Classes: ['agri', 'barrenland', 'grassland', 'urban']
Class distribution:
  agri        :   1600 (25.00%)
  barrenland  :   1600 (25.00%)
  grassland   :   1600 (25.00%)
  urban       :   1600 (25.00%)
```

## ğŸ¯ Co zrobiÄ‡ dalej?

### 1. Retrenuj model z nowymi ustawieniami

```bash
# WyczyÅ›Ä‡ stare logi
just clear-logs

# Trenuj z poprawionymi ustawieniami
just train KAN_FAST

# Monitoruj w TensorBoard
just tensorboard
```

### 2. Obserwuj metryki podczas treningu

**Oczekiwane zachowanie:**
- Validation accuracy powinna byÄ‡ niÅ¼sza niÅ¼ poprzednio (~85-90%)
- Test accuracy powinna byÄ‡ zbliÅ¼ona do validation accuracy (Â±5%)
- JeÅ›li nadal duÅ¼a rÃ³Å¼nica â†’ moÅ¼liwy distribution shift

### 3. Eksperymenty z regularizacjÄ…

JeÅ›li problem persystuje, sprÃ³buj:

```bash
# ZwiÄ™ksz augmentacjÄ™
# Zmniejsz learning rate
uv run train model_params.learning_rate=5e-5

# Zmniejsz liczbÄ™ epok (early stopping)
uv run train model_params.max_epochs=20

# ZmieÅ„ optimizer
uv run train model_params.optimizer=adamw
```

### 4. PorÃ³wnaj rÃ³Å¼ne modele

```bash
# Test z innymi architekturami
just train resnet50
just train efficientnet_b0
just train vit_b_16

# Zobacz ktÃ³ry generalizuje najlepiej
just test-last resnet50
just test-last efficientnet_b0
```

## ğŸ“ˆ Oczekiwane wyniki po poprawkach

| Metryka | Przed | Po poprawkach | Cel |
|---------|-------|---------------|-----|
| Train Accuracy | ~99% | ~95-98% | âœ“ Mniej overfitting |
| Val Accuracy | ~98% | ~85-92% | âœ“ Bardziej realistyczne |
| Test Accuracy | ~35% | ~80-90% | âœ“ Dobra generalizacja |
| Val/Test gap | 63% | <10% | âœ“ Konsystencja |

## ğŸ”§ Dodatkowe techniki

### 1. Early Stopping

Dodaj do train.py:

```python
from lightning.pytorch.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=5,
    mode="min",
    verbose=True,
)
```

### 2. Learning Rate Scheduler

```python
def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    return {
        "optimizer": optimizer,
        "lr_scheduler": {
            "scheduler": scheduler,
            "monitor": "val_loss",
        },
    }
```

### 3. Mixup / CutMix augmentacja

Dla bardzo trudnych przypadkÃ³w, rozwaÅ¼ zaawansowane augmentacje.

## ğŸ“ Podsumowanie

**Problem byÅ‚ spowodowany przez:**
1. âœ— Brak seed w random_split â†’ niereprodukowalny podziaÅ‚
2. âœ— Brak augmentacji â†’ overfitting
3. âœ— Model "znaÅ‚" rozkÅ‚ad danych treningowych zbyt dobrze

**RozwiÄ…zanie:**
1. âœ“ Fixed seed dla reprodukowalnoÅ›ci
2. âœ“ Augmentacja danych dla treningu
3. âœ“ Osobne transformacje dla train/val
4. âœ“ KonfigurowalnoÅ›Ä‡ przez Hydra
5. âœ“ NarzÄ™dzia do analizy (analyze_data.py)

**NastÄ™pne kroki:**
1. Uruchom `just analyze-data` aby zdiagnozowaÄ‡ dane
2. Retrenuj model z nowymi ustawieniami
3. PorÃ³wnaj validation i test accuracy
4. Dostosuj hyperparametry jeÅ›li potrzeba
